---
title: Dealing with Overdispersion
author: Chris Mainey
date: '2019-10-18'
slug: dealing-with-overdispersion
categories: 
- ovedispersion
- R
draft: true
tags: 
- R
- overdispersion
- count data
- mixed models
summary: Overdispersion is a phenomenon affecting count data.  This post explains what it is, and how to adjust for it.
bibliography: ../../content/post/references.bib
image:
  caption: ''
  focal_point: ''
math: true
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
dev.args = list(png = list(type = "cairo"))
)
library(NHSRdatasets)
library(Cairo)
```

# What is overdispersion?

This post discusses 'overdispersion' ('OD' in this post), a term commonly seen when using count data (such as the NHS mortality indicator [SHMI](www.digital.nhs.uk/SHMI).  I will discuss what it is, how it affects our data and our interpretations as well as some options for dealing with it.  It is related to an earlier post on [Funnel plots](https://mainard.co.uk/post/introduction-to-funnel-plots/), as these plot need adjustment in the presence of OD.  It will help explain the adjustment used in that post, as well as other (and in my opinion, better) approaches.

To understand the context, we have to think about count data.  Count data are real, numeric values (sometimes referred to as 'discrete'), they can only be whole numbers.  They are bounded at (cannot go below) zero.  So we can have a count of 2 or 3, but not 2.5, and we can have 0 or more events, but not -1 events.

These properties of counts mean that they are naturally skewed (the distribution is not symetrical) when the mean count is low.  They are therefore not normally distributed (see below).  

```{r distributions}
set.seed(123)
dist <- data.frame(Normal1 = rnorm(1000, 1, 2),
                   Normal2 = rnorm(1000, 2, 2),
                   Normal5 = rnorm(1000, 5, 2),
                   Normal10 = rnorm(1000, 10, 2),
                   Poisson1 = rpois(1000, 1),
                   Poisson2 = rpois(1000, 2),
                   Poisson5 = rpois(1000, 5),
                   Poisson10 = rpois(1000, 10)
)

library(ggplot2)                   
library(tidyr)

dist<-gather(dist, key="key", value="value")
dist$key <- factor(dist$key, ordered = TRUE)
dist$key <- forcats::fct_inorder(dist$key)
dist$dist <- as.integer(stringr::str_detect(dist$key, "Normal"))

ggplot(dist, aes(x=value, col=key, fill = key))+
  geom_histogram(alpha=0.5,position = "identity")+
  facet_wrap(~dist)

```



They are commonly modelled using the Poisson distribution.  Forgive the mathsy description, but it's the best way:

$$ Pr(Y = y) = \frac{\mu^y e^-\mu^\lambda }{y!}$$


When transformed into a Poisson regression, we end up modelling the expected count $\mu$, ususllay through a log transformation:

$$ ln(\mu) = \alpha + \beta x$$

In the equation above we have no separate error term, and size of $\mu$ is also it's variance.
This means that the variance cannot be different.  Unless we have perfectly behaved Poisson data, and few things in real life are perfectly behaved, our variance will not be quite right.  The upshot of this is that the assessment of parameter signficance, and model accuracy will be affected.

We usually observe 'overdispersion', where there is more variance in our data than the Poisson distribtuion assumes.  This means that we underestiamte our error, and overestimate the signficance of parameters.  There are a lot of different way to deal with this.  Our choice of technique depends on our assumptions, the data generating mechinsm and what we want to use the model for.

Overdispersion generally arises from one of the following:

+ Insufficent predictors in the model (bad model)
+ Poorly parameterised model (bad preparation)
+ Aggregation of data
+ Correlations in data / violations of the 'independence' assumption

1 & 2 can be solved by having more, and better quality data.  Sometimes this is possible, but often it's not.  Aggreagation is sometimes neccisary because we don't have the underlying data, or we are building panel models based on counts etc.  


# Exammple:

I'm going to use the `NHSRdatasets` package for this post, and build a regression model on the Length-of-Stay field in the `LOS_model` data set.  we can then test if it is over dispersed.  Firstly, lets visulaise the distribution before we build our model.

```{r inst, eval=FALSE}
install.packages("NHSRdatasets")
library(NHSRdatasets)
```

```{r load, warning=FALSE, message=FALSE}
data("LOS_model")

Hmisc::describe(LOS_model)

library(ggplot2)
ggplot(LOS_model, aes(LOS))+
  geom_histogram(bins=15, fill="dodgerblue2", alpha=0.6, col=1)+
  geom_vline(aes(xintercept=median(LOS)), col="red")+
  ggtitle("Histogram of Length of Stay in 'LOS_model'")
```

So we have a skewed distribution, with the median represente dby the red line.  Lets build a Poisson model next:

```{r glm}
glm1 <- glm(LOS ~ Age * Death, data=LOS_model, family="poisson")

summary(glm1)
```


We can test for OD, using the ratio of the sum of the squared Pearson residuals over the degrees of freedom.  If this value is 1, then the variance is as expected ('equidispersion'), but if it is  >1, then we have OD, and this value can be interpreted as a dispersion ratio (\u03D5):
```{r od}
phi <- sum(residuals(glm1,type="pearson") ^2)/ df.residual(glm1)
phi
```

Our data are showing OD, but it is not extreme, with 1.46 times the expected variance.  The models used during my PhD work had dispersion ratios of >40, which forced me to figure out how to deal with it.


# Methods for dealing with OD

Where it's not possible to change our data structure and processing rules, we can try some of the following:

1. Ignore it! 
1. Scale the variance an manually re-calculate
1. Quasi-likelihood models
1. Mixture models:
 1. Zero-inflated models
 1. Negaive Binomial models
1. Models for clusterd data:
 1. Generalized Estimating equations
 1. Random-Intercept models
 
 
## Ignore it:

The Poisson distribution is (helpfully) known to be unbiased, so unless we need to considered the signficance of parameters, we might be fine ignoring it.  If our data doesn't violate regression assumptions, and we are using it for predictions or parameter estimates, we are fine.

## Scale the variance manually

You could extract the variance of the parameters estimated and multiply them by the scale parameter.  We'll use confidence intervals fro comparison here, which are essentially:  $coefficient \pm 1.96 * Standard Error$. To get this from our model, remeber that the SE's are the square-root of the variance, so use SE^2 * \u03D5:

```{r scalevar}

scalederr <- sqrt(summary(glm1)$coefficients[,2]^2 * phi)

old_CI <- confint.default(glm1, type="wald")

lci <- coef(glm1) - (1.96 * scalederr)
uci <- coef(glm1) + (1.96 * scalederr)

cbind(old_CI, "New LCI"= lci, "New UCI" = uci)

```

## Quasi-likelihood models


To fit a quasi-Poisson model, we simply change the family in the `glm`.  Note the dispersion parameter matches the calculation above, and the standard errors are inflated to account for this:
```{r quasi}
quasi<-glm(LOS ~ Age * Death, data=LOS_model, family="quasipoisson" )

summary(quasi)

confint(quasi)
```

## Negative Binomial Model

Another option is to use more than one distribution to model the data. There are a few options for this, but the Negative binomial family is one.  THis is a mixture of disributions, and 'nb1' is similar to the quasi likelihood below, but 'nb2' is a mixutre of Poisson means that are, themselves, gamma-distributed.  There are some assumptions beneath this method, but essentially we assume there is more overdispersion at the smaller means (clusters etc.).

For a negative binomial model, two options are `glm.nb` from the `MASS` package, or `glmmTMB` function from the package of the same name.  Note that the function from `MASS` does not take a `family` argument.  The dispersion factor here is quadratic to the mean, so don't compare it directly to the scale factor \u03D5 above.
```{r nb, warning=FALSE, message=FALSE}
library(MASS)

nb <- glm.nb(LOS ~ Age * Death, data=LOS_model)

summary(nb)

confint(nb)

```

## Generalized Estimating Equations

If you think you OD is noise due to clustering, you can factor it out with GEEs.  GEEs do not se full-maximum likelihood, and are more like the quasilikelihood models above than the random effects models below, but they predict at the global mean, or 'population average.'  This is useful if you have something like a national dataset with regional data.  You might  want to filter out the regional clustering and view the results at the population level.

```{r gee}
library(gee)
gee1 <-   glm(LOS ~ Age * Death, data=LOS_model, family="poisson")

summary(gee1)
```


## Random-effects models: Random Intercept

Finally we could use a random-effects model.  They have various names depending on the discipline:  multilevel models, heirarchical models, generalized linear mixed model (GLMM) etc., but the all imply that we can build a model with more than one source of variance.  In the our example case, we can reasonbly assume that we might have variation within each trust and between each trust.  Said another way, we have a distribution within each trust (or cluster), and those clusters are, themselves, normally distributed about the global mean.  Normal `glm` and `lm` models assume all points are independent, but all data points at organisation A are a little more like each other (due to dmeographics, treatment patterns, locla factors etc.) than like the national average.  If we still calculate the global model, but we allow the intercept to vary for each cluster, we have a random-intercept model.  These models therefore have standard regresison coefficients, and a global intercept, but an additional variance term for the intercpet and (burried in the model object) an intercept value for each cluster.

We will fit a random random-intercept model with `glmer` from the `lme4` package.  There are many different packages for doing this in `R`, and it's an area where "Bayesian" and "Frequentest" methods differ in theory but share many of the same processes in practice.  I also like the `glmmTMB` package, as it has more distributions, and is faster than `lme4`, but it is not yet as mature.

The extra formula argument in the model below: (1|Organisation), specifies the random-intercept, where 1 means the intercept, and the `|` can be read as 'by', in this case, by organisation.  Multi-level models are huge field in and of themselves, but the main points takeaways here are: 
- Understanding the random-intercept structure
- How this changes your interpretation of estimates
- To note that you can predict with or without the random effects (see below)
- These models are very sensitive to scale.  For scaling I use the `scale` and `arm::rescale` functions (and Andrew Gelman has something to say on this:  http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf)

```{r glmm, warning=FALSE, message=FALSE}
library(lme4)

glmm <- glmer(LOS ~ scale(Age) * Death + (1|Organisation), data=LOS_model, family="poisson")

summary(glmm)
```

```{r confint}
confint(glmm)

```

These models are significantly more complicated than the ones shown previously.  There are some philosophical issues with standard errors of random effects (see Ben Bolker's amazing GLMM FAQs: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html). 

`Confint` can be used to calculate the confidence interval above by profiling the likelihood (provided you have the `MASS` package installed).  This allows you to assess the significance of the random effect.  Our CI butts up against zero here, and the AIC is slightly worse than the `glm`.  This suggests it is not well modelled with the normally distributed random intercept.  The NB model appears to perform better (and is essential a gamma-distributed random-intercept, but we'll save that for another day...).

### Predictions from GLMMs
Predictions from GLMMS also require more thought.  You can predict with the random effects ("conditional") or without ("marginal").  Conditional predictions will have the adjustment for their cluster (or whatever random effect you have fitted), but marginal will not, and could be considered the 'model average' in the case above.  Subject to the fixed effects, removing the effects of cluster (e.g. risk for the average patient/unit with that exposure, regardless of where they were seen) allows prediction at the 'average rate.'



# So which was better?

Unfortunatley, it's not that simple.  It really depends on what you are trying to do and the assumptions you make.  You may be happy scaling the variance to assess the signifcance of predictors.  You may want accurate global ("marginal") or cluster-specific ("conditional") predictions.

You can do things like:
+ Compare models after bootstrapping (refitting to models to many sub-samples, with replacement)
+ Use AIC between models of the same class: although this falls down when comparing single-level models and multilevel models due to boundary constrainst (again, save for another day...)

I would suggest that you try a few of them, but think about your output and what it represents.  During my PhD work, I settled on a final model that used both a random-intercept for hospital-specific clustering combined with a negative binomial family that better adjusted for the unequal OD due to the size of the aggreagated units in my dataset.  It's not easy to get your head around, but it's not impossible.  Have a go and see what you think!


## References

1. CAMERON, A. C. & TRIVEDI, P. K. 2013. _Regression Analysis of Count Data_, Cambridge University Press

1. GOLDSTEIN, H. 2010. _Multilevel Statistical Models_, John Wiley & Sons Inc.

1. GELMAN, A. & HILL, J. 2006. _Multilevel structures_. In: GELMAN, A. & HILL, J. (eds.) _Data Analysis Using Regression and Multilevel/Hierarchical Models_. Cambridge: Cambridge University Press.

1. GELMAN, A. 2008. _Scaling regression inputs by dividing by two standard deviations_. Statistics in Medicine, 27, 2865-2873.