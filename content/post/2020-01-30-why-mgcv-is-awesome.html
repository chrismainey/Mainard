---
title: "Why mgcv is awesome"
author: ''
date: '2020-01-31'
slug: why-mgcv-is-awesome
categories:
- package
tags:
- R
- package
- GAM
- spine
summary: "Why building Generalized Additive Models (GAMs) using the `mgcv` package is a great idea."
image:
  caption: ''
  focal_point: ''
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 1
draft: false
math: true
---


<div id="TOC">
<ul>
<li><a href="#why-mgcv-is-awesome"><span class="toc-section-number">1</span> Why <code>mgcv</code> is awesome</a></li>
<li><a href="#regression-models"><span class="toc-section-number">2</span> Regression models</a></li>
<li><a href="#what-about-nonlinear-relationships"><span class="toc-section-number">3</span> What about nonlinear relationships?</a></li>
<li><a href="#splines"><span class="toc-section-number">4</span> Splines</a></li>
<li><a href="#how-smooth-is-smooth"><span class="toc-section-number">5</span> How smooth is smooth?</a></li>
<li><a href="#generalized-additive-models-gams"><span class="toc-section-number">6</span> Generalized Additive Models (GAMs)</a></li>
<li><a href="#mgcv-mixed-gam-computation-vehicle"><span class="toc-section-number">7</span> mgcv: mixed gam computation vehicle</a></li>
<li><a href="#model-output"><span class="toc-section-number">8</span> Model Output:</a></li>
<li><a href="#check-your-model"><span class="toc-section-number">9</span> Check your model:</a></li>
<li><a href="#is-it-any-better-than-linear-model"><span class="toc-section-number">10</span> Is it any better than linear model?</a></li>
<li><a href="#summary"><span class="toc-section-number">11</span> Summary</a></li>
<li><a href="#references"><span class="toc-section-number">12</span> References:</a></li>
</ul>
</div>

<div id="why-mgcv-is-awesome" class="section level1">
<h1><span class="header-section-number">1</span> Why <code>mgcv</code> is awesome</h1>
<p><img src="static/img/download.png"></p>
<p><em>Donâ€™t think about it too hardâ€¦</em> ðŸ˜‰</p>
<p>This post looks at why building Generalized Additive Model, using the <code>mgcv</code> package is a great option. To do this, we need to look, first, at linear regression and see why it might not be the best option in some cases. Iâ€™m using simulated data for all plots, but Iâ€™ve hidden the first few code chunks to aid the flow of the post. If you want to see them, you can get the code from the page on my GitHub repository:</p>
<p><br><br></p>
</div>
<div id="regression-models" class="section level1">
<h1><span class="header-section-number">2</span> Regression models</h1>
<p>Letâ€™s say we have some data with two attributes, <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. If they are linearly related, they might look something like this:</p>
<pre class="r"><code>library(ggplot2)

a&lt;-ggplot(my_data, aes(x=X,y=Y))+
  geom_point()+
  scale_x_continuous(limits=c(0,5))+
  scale_y_continuous(breaks=seq(2,10,2))+
  theme(axis.title.y = element_text(vjust = 0.5,angle=0))

a</code></pre>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/regression1-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>To examine this relationship we coudl use a regression model. Linear regression is a method for predicting a variable, <span class="math inline">\(Y\)</span>, using another, <span class="math inline">\(X\)</span>. Applying this to our data would predict a set of values that form the red line:</p>
<pre class="r"><code>library(ggforce)
a+geom_smooth(col=&quot;red&quot;, method=&quot;lm&quot;)+
  geom_polygon(aes(x=X, y=Y), col=&quot;goldenrod&quot;, fill=NA, linetype=&quot;dashed&quot;, size=1.2, data=dt2)+
  geom_label(aes(x=X, y = Y, label=label), data=triangle)+
  geom_mark_circle(aes(x=0, y=2), col=&quot;goldenrod&quot;,  fill=NA, linetype=&quot;dashed&quot;, size=1.2)+
  theme(axis.title.y = element_text(vjust = 0.5,angle=0))</code></pre>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/regression2-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>This might bring back school maths memories for some, but it is the â€˜equation of a straight lineâ€™. According to this equation, we can describe a striaght line from the position it starts on the <span class="math inline">\(y\)</span> axis (the â€˜interceptâ€™, or <span class="math inline">\(\alpha\)</span>), and how much <span class="math inline">\(y\)</span> increases for each unit of <span class="math inline">\(x\)</span> (the â€˜slopeâ€™, which we will also call the coefficeint of <span class="math inline">\(x\)</span>, or <span class="math inline">\(\beta\)</span>). There is also a some natural flutation, as all points would be perfectly in-line if not. We refer to this as the â€˜residual errorâ€™ (<span class="math inline">\(\epsilon\)</span>). Phrased mathematically, that is:</p>
<p><span class="math display">\[y= \alpha + \beta x + \epsilon\]</span></p>
<p>Or if we subsitute the actual figures in, we get the following:</p>
<p><span class="math display">\[y= 2 + 1.5 x + \epsilon\]</span></p>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/regression3-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>This post wonâ€™t go into the mechanics of estimating these models in any depth, but we estimate the models by taking the difference between each data point and the line (the â€˜residualâ€™ error), then minimising this difference. We have both positive and negative errors, above and below the line, so to make them all positive for estimation by squaring them and minimising the â€˜sum of the squares.â€™ You may hear this referred to as â€˜ordinary least squaresâ€™, or OLS.</p>
<p><br><br></p>
</div>
<div id="what-about-nonlinear-relationships" class="section level1">
<h1><span class="header-section-number">3</span> What about nonlinear relationships?</h1>
<p>So what do we do if our data look more like this:</p>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/sig-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>One of the key assumptions of the model weâ€™ve just seen is that <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are linearly related. If our <span class="math inline">\(y\)</span> is not normally distributed, we use a Generalized Linear Model <em>(Nelder &amp; Wedderburn, 1972)</em>, where <span class="math inline">\(y\)</span> is transformed through a link-function, but again we are assuming <span class="math inline">\(f(y)\)</span> and <span class="math inline">\(x\)</span> are linearly related. If this is not the case, and the relationship varies across the range of <span class="math inline">\(x\)</span>, it may not be the best fit. We have a few options here:</p>
<ul>
<li>We can use a linear fit, but we will under or over score sections of the data if we do that.</li>
<li>We can devide into categories. Iâ€™ve used three in the plot below, and that is a reasonable option, but itâ€™s a bit or a â€˜broad brush stroke.â€™ Again we may be under or over score sections, plus is seems inequiatble around the boundaries between categories. e.g.Â is <span class="math inline">\(y\)</span> that much different if <span class="math inline">\(x=49\)</span> when compared to <span class="math inline">\(x=50\)</span>?</li>
<li>We can use transformation such as polynomials. Below Iâ€™ve used a cubic polynomial, so the model fits: <span class="math inline">\(y = x + x^2 + x^3\)</span>. The combination of these allow the functions to smoothly approximate changes. This is a good option, but can oscilate at the extremes and may induce correlations in the data that degreade the fit.</li>
</ul>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/cats-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br><br></p>
</div>
<div id="splines" class="section level1">
<h1><span class="header-section-number">4</span> Splines</h1>
<p>A further refinement of polynomials is to fit â€˜piece-wiseâ€™ polynomials, where we chain polynomials together across the range of the data to describe the shape. â€˜Splinesâ€™ are piecewise polynomials, named after the tools draftsmen used to use to draw curves. Physical splines were flexible strips that could bent to shape and were held in place by weights. When constructing mathmetical splines, we have polynomial functions (the flexible strip), continuous up to and including second derivative (for those of you who know what that means), fixed together at â€˜knotâ€™ points.</p>
<p>Below is a <code>ggplot2</code> object with a <code>geom_smooth</code> line with a formula containing a â€˜natural cubic splineâ€™ from the <code>ns</code> function. This type of spline is â€˜cubicâ€™ (<span class="math inline">\(x+x^2+x^3\)</span>) and linear past the outer knot points (â€˜naturalâ€™), and it is using 10 knots</p>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/gam1-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br><br></p>
</div>
<div id="how-smooth-is-smooth" class="section level1">
<h1><span class="header-section-number">5</span> How smooth is smooth?</h1>
<p>The splines can be a smooth or â€˜wigglyâ€™ as you like, and this can be controlled either by altering the number of knots <span class="math inline">\((k)\)</span>, or by using a smoothing penalty <span class="math inline">\(\gamma\)</span>. If we increase the number of knots, it will be more â€˜wigglyâ€™. This is likely to be closer to the data, and the error will be less, but we are starting to â€˜overfitâ€™ the relationship and fit the noise in our data. When we combine the smoothing penalty, we penalize the complexity in the model and this helps reduce the overfitting.</p>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/knots2-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br><br></p>
</div>
<div id="generalized-additive-models-gams" class="section level1">
<h1><span class="header-section-number">6</span> Generalized Additive Models (GAMs)</h1>
<p>A Generalized Additive Model (GAM) <em>(Hastie, 1984)</em> uses smooth functions (like splines) for the predictors in a regression model. These models are stricly additive, meaning we canâ€™t use interaction terms like a normal regression, but we could achieve the same thing by reparametrising as a smoother. This is not quite the case, but essentially we are moving to a model like:</p>
<p><span class="math display">\[y= \alpha + f(x) + \epsilon\]</span></p>
<p>A more formal example of a GAM, taken from Wood <em>(2017)</em>, is:</p>
<p><span class="math display">\[g(\mu i) = A_i \theta + f_1(x_1) + f_2(x_{2i}) + f3(x_{3i}, x_{4i}) + ...\]</span>
<br></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\mu i \equiv E(Y _i)\)</span>, the expectation of Y</p></li>
<li><p><span class="math inline">\(Yi \sim EF(\mu _i, \phi _i)\)</span>, <span class="math inline">\(Yi\)</span> is a response variable, distributed according to exponential family distribution with mean <span class="math inline">\(\mu _i\)</span> and shape parameter <span class="math inline">\(\phi\)</span>.</p></li>
<li><p><span class="math inline">\(A_i\)</span> is a row of the model matrix for any strictly parametric model components with <span class="math inline">\(\theta\)</span> the corresponding parameter vector.</p></li>
<li><p><span class="math inline">\(f_i\)</span> are smooth functions of the covariates, <span class="math inline">\(xk\)</span>, where <span class="math inline">\(k\)</span> is each function basis.</p></li>
</ul>
<div id="so-what-does-that-mean-for-me" class="section level2">
<h2><span class="header-section-number">6.1</span> So what does that mean for me?</h2>
<p>In cases where you would build a regression model, but you suspect a smooth fit would do a better job, GAM are a great option. They are suited to non-linear, or noisy data, and you can use â€˜random effectsâ€™ as they can be viewed as a type of smoother, or all smooths can be reparameterised as random effects. In a similar fashion, they can be viewed as either Frequentist random effects models, or as Bayesian models where the smoother is essentially your prior (donâ€™t quote me on that, I know very little about Bayesian methods at this point).</p>
<p>There are two major implmentations in <code>R</code>:</p>
<ul>
<li><p>Trevor Hastieâ€™s <code>gam</code> package, that uses loess smoothers or smoothing splines at every point.</p></li>
<li><p>Simon Woodâ€™s <code>mgcv</code> package, that uses reduced-rank smoothers, and is the subject of this post.</p></li>
</ul>
<p>There are many other further advances on these approaches, such as GAMLSS, but they are beyond the scope of this post.</p>
<p><br><br></p>
</div>
</div>
<div id="mgcv-mixed-gam-computation-vehicle" class="section level1">
<h1><span class="header-section-number">7</span> mgcv: mixed gam computation vehicle</h1>
<p>Prof.Â Simon Woodâ€™s package <em>(Wood,2017)</em> is pretty much the standard. It is included in the standard distribution of <code>R</code> from the <code>R</code> project, and included in other packages such as in <code>geom_smooth</code> from <code>ggplot2</code>. I described it above as using â€˜reduced-rankâ€™ smoothers. By this, I mean that we donâ€™t fit a smoother to every point. If our data has 100 points, but could be adequatley described by a smoother with 10 knots, it could be dscribed as wasteful to require more. This also hits our degrees-of-freedom and can affect our tendency to overfit. When we combine enough knots to adequatley describe our dataâ€™s relationship, we can use the penalty to hone this to the desired shape. This is a nice safety net, as the number of knots is not critical past the minimum number.</p>
<p><code>mgcv</code> not only offers the mechanism to build the models and smoothers, but it will also automatically estimate the penalty values for you and optimize the smoothers. When you are more familiar with it, you can change these settings, but it does a very good job out of the box. In my own PhD work, I was building models based on overdispersed data, where there was more error/noise in the data than expected. <code>mgcv</code> was great at cutting through this noise, but I had to increase the penalty values to compensate for this noise.</p>
<p>So how do you specify an <code>mgcv</code> GAM for the sigmoidal data above? Here I will use a cubic regression spline in <code>mgcv</code>:</p>
<pre class="r"><code>library(mgcv)
my_gam &lt;- gam(Y ~ s(X, bs=&quot;cr&quot;), data=dt)</code></pre>
<p>The settings above mean:</p>
<ul>
<li><p><code>s()</code> specifies a smoother. There are other options, but <code>s</code> is a good default</p></li>
<li><p><code>bs="cr"</code> is telling it to use a cubic regression spline (â€˜basisâ€™). Again there are other options, and the default is a â€˜thin-plate regression splineâ€™, which is a little more complicated than the one above, so I stuck with <code>cr</code>.</p></li>
<li><p>The <code>s</code> function works out a default number of knots to use, but you can alter this with <code>k=10</code> for 10 knots for example.</p></li>
</ul>
<p><br><br></p>
</div>
<div id="model-output" class="section level1">
<h1><span class="header-section-number">8</span> Model Output:</h1>
<p>So if we look at our modle summary, as we would do with a regression model, weâ€™ll see a few things:</p>
<pre class="r"><code>summary(my_gam)</code></pre>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## Y ~ s(X, bs = &quot;cr&quot;)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  43.9659     0.8305   52.94   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##        edf Ref.df     F p-value    
## s(X) 6.087  7.143 296.3  &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.876   Deviance explained = 87.9%
## GCV = 211.94  Scale est. = 206.93    n = 300</code></pre>
<ul>
<li>Model coefficients for our intercept is shown, and any non-smoothed parameters will show here</li>
<li>An overall signficance of each smooth term is below.</li>
<li>Degress of freedom look unusual, as we have decimal. This is based on â€˜effective degrees of freedomâ€™ (edf), because we are using spline functions that expand into many parameters, but we are also penalising them and reducing their effect. We have to estimate our degress of freedom rather than count predictors <em>(Hastie et al., 2009)</em> .</li>
</ul>
<p><br><br></p>
</div>
<div id="check-your-model" class="section level1">
<h1><span class="header-section-number">9</span> Check your model:</h1>
<p>The <code>gam.check()</code> function can be used to look at the residual plots, but it also test the smoothers to see if there are enough knots to describe the data. Read the details below, or the help file, but if the p-value is very low, you need more knots.</p>
<pre class="r"><code>gam.check(my_gam)</code></pre>
<p><img src="/post/2020-01-30-why-mgcv-is-awesome_files/figure-html/gamcheck-1.png" width="960" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Method: GCV   Optimizer: magic
## Smoothing parameter selection converged after 4 iterations.
## The RMS GCV score gradient at convergence was 1.107369e-05 .
## The Hessian was positive definite.
## Model rank =  10 / 10 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k&#39;.
## 
##        k&#39;  edf k-index p-value
## s(X) 9.00 6.09     1.1    0.97</code></pre>
<p><br><br></p>
</div>
<div id="is-it-any-better-than-linear-model" class="section level1">
<h1><span class="header-section-number">10</span> Is it any better than linear model?</h1>
<p>Lets test our model against a regular linear regression withe the same data:</p>
<pre class="r"><code>my_lm &lt;- lm(Y ~ X, data=dt)

anova(my_lm, my_gam)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Y ~ X
## Model 2: Y ~ s(X, bs = &quot;cr&quot;)
##   Res.Df   RSS     Df Sum of Sq      F    Pr(&gt;F)    
## 1 298.00 88154                                      
## 2 292.91 60613 5.0873     27540 26.161 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yes, yes it is! Our anova function has perfromed an f-test here, and our GAM model is significantly better that linear regression.</p>
</div>
<div id="summary" class="section level1">
<h1><span class="header-section-number">11</span> Summary</h1>
<p>So, we looked at what a regression model is and how we are explaining one variable: <span class="math inline">\(y\)</span>, with another: <span class="math inline">\(x\)</span>. One of the underlying assumptions is a linear relationship, but this isnâ€™t always the case. Where the realtionship varies across the range of <span class="math inline">\(x\)</span>, we can use functions to alter this shape. A nice way to do it is by chaining smooth curves together at â€˜knotâ€™-points, and we refer to this as a â€˜spline.â€™</p>
<p>We can use these splines in a regular regression, but if we use them in the context of a GAM, we estimate both the regression model and how smooth to make our smoothers.</p>
<p>The <code>mgcv</code> package is great for estimating GAMs and choosing the smoothing parameters. The example above shows a spline-based GAM giving a much better fit than a linear regresison model. If your data are not linear, or noisy, a smoother might be appropriate</p>
<p>I highly recommend Noam Rossâ€™ <a href="@noamross">https://twitter.com/noamross?s=20</a> free online GAM course: <a href="https://noamross.github.io/gams-in-r-course/" class="uri">https://noamross.github.io/gams-in-r-course/</a></p>
<p>For more depth, Simon Woodâ€™s Generalized Additive Models is one of the best books on, not just GAMS, but regression in general: <a href="https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331" class="uri">https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331</a></p>
<p><br><br></p>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">12</span> References:</h1>
<ul>
<li><p>NELDER, J. A. &amp; WEDDERBURN, R. W. M. 1972. Generalized Linear Models. Journal of the Royal Statistical Society. Series A (General), 135, 370-384.</p></li>
<li><p>HARRELL, F. E., JR. 2001. Regression Modeling Strategies, New York, Springer-Verlag New York.</p></li>
<li><p>HASTIE, T. &amp; TIBSHIRANI, R. 1986. Generalized Additive Models. Statistical Science, 1, 297-310. 291</p></li>
<li><p>HASTIE, T., TIBSHIRANI, R. &amp; FRIEDMAN, J. 2009. The Elements of Statistical Learning : Data Mining, Inference, and Prediction, New York, NETHERLANDS, Springer.</p></li>
<li><p>WOOD, S. N. 2017. Generalized Additive Models: An Introduction with R, Second Edition, Florida, USA, CRC Press.</p></li>
</ul>
</div>
